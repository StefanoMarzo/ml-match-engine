{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b1919c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "becc6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "from rank_bm25 import BM25L\n",
    "\n",
    "# Pandas config\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a861be",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f873c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_not_matched = pd.read_parquet('title_not_matched.parquet')\n",
    "offers_training_df = pd.read_parquet('offers_training.parquet')\n",
    "offers_test_df = pd.read_parquet('offers_test.parquet')\n",
    "matches_training_df = pd.read_parquet('matches_training.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e249c",
   "metadata": {},
   "source": [
    "### Process text for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ea32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = []\n",
    "with open('german_stopwords.txt', \"r\") as f:\n",
    "    stop_word_list = f.read().split()\n",
    "class TextTransformer:\n",
    "    def processed_text(self, text):\n",
    "        if text is None:\n",
    "            return ''\n",
    "        #lower\n",
    "        processed = text.lower()\n",
    "        #remove accents\n",
    "        processed = self.simplify(processed)\n",
    "        #remove special characters\n",
    "        processed = ''.join(c if c.isalnum() or c == ' ' else ' ' for c in processed)\n",
    "        #remove unnecessary double spaces\n",
    "        processed = re.sub(' +', ' ', processed)\n",
    "        #strip\n",
    "        processed = processed.strip()\n",
    "        #remove stopwords\n",
    "        processed_list = [word for word in processed.split() if word not in stop_word_list]\n",
    "        return ' '.join(processed_list)\n",
    "    \n",
    "    def simplify(self, text):\n",
    "        try:\n",
    "            text = unicode(text, 'utf-8')\n",
    "        except NameError:\n",
    "            pass\n",
    "        text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "        return str(text)\n",
    "with open('color_processing/kfix_de.txt') as f:\n",
    "    color_kfix_list = f.read().splitlines() \n",
    "\n",
    "with open('color_processing/primary_colors_de.txt') as f:\n",
    "    primary_colors = f.read().splitlines() \n",
    "color_matches = {}\n",
    "with open('color_processing/color_matches_de.txt') as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split(':')\n",
    "        val = val.strip()\n",
    "        color_matches[key] = val\n",
    "with open('title_processing/kfix_de.txt') as f:\n",
    "    clothes_kfix_list = f.read().splitlines() \n",
    "\n",
    "with open('title_processing/primary_clothes_de.txt') as f:\n",
    "    primary_clothes = f.read().splitlines() \n",
    "clothes_matches = {}\n",
    "with open('title_processing/clothes_matches_de.txt') as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split(':')\n",
    "        val = val.strip()\n",
    "        clothes_matches[key] = val\n",
    "class SpecificTrasformer(TextTransformer):\n",
    "    def __init__(self,\n",
    "                 kfix_list, \n",
    "                 matches, \n",
    "                 primary):\n",
    "        self.kfix_list = kfix_list\n",
    "        self.matches = matches\n",
    "        self.primary = primary\n",
    "    def separe_word(self, text, word):\n",
    "        i = text.find(word)\n",
    "        text = text[:i] + ' ' + text[i:] if i != -1 else text\n",
    "        return text[:i+len(word)+1] + ' ' + text[i+len(word)+1:] if i != -1 else text\n",
    "    def separe_words(self, text):\n",
    "        for color in self.primary:\n",
    "            text = self.separe_word(text, color)\n",
    "        return text\n",
    "    def replace_words(self, text):\n",
    "        for k, v in self.matches.items():\n",
    "            text = text.replace(k, v)\n",
    "        return text\n",
    "    def remove_kfix(self, text):\n",
    "        for suffix in self.kfix_list:\n",
    "            text = text.replace(suffix, '')\n",
    "        return text\n",
    "    \n",
    "    def processed_text(self, text):\n",
    "        splitted = super().processed_text(text).split()\n",
    "        #1 transform matches\n",
    "        splitted = [self.replace_words(text) for text in splitted]\n",
    "        #2 suffix removal\n",
    "        splitted = [self.remove_kfix(text) for text in splitted]\n",
    "        #separate primary colors\n",
    "        splitted = [self.separe_words(term) for term in splitted]\n",
    "        return re.sub(' +', ' ', ' '.join(splitted).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123da77",
   "metadata": {},
   "source": [
    "### Instance of text transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f0f3ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TextTransformer()\n",
    "ct = SpecificTrasformer(color_kfix_list, color_matches, primary_colors)\n",
    "titlet = SpecificTrasformer(clothes_kfix_list, clothes_matches, primary_clothes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a6357",
   "metadata": {},
   "source": [
    "### Clean DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zalando_prod_training = offers_training_df.loc[offers_training_df['shop'] == 'zalando']\n",
    "zalando_prod_training = zalando_prod_training\\\n",
    "                    .loc[zalando_prod_training['offer_id'].isin(matches_training_df['zalando'])]\n",
    "zalando_prod_training['brand'] = zalando_prod_training['brand']\\\n",
    "                    .apply(lambda x: tt.processed_text(x))\n",
    "zalando_prod_training['title'] = zalando_prod_training['title']\\\n",
    "                    .apply(lambda x: titlet.processed_text(x))\n",
    "zalando_prod_training['color'] = zalando_prod_training['color']\\\n",
    "                    .apply(lambda x: ct.processed_text(x))\n",
    "\n",
    "aboutyou_prod_training = offers_training_df.loc[offers_training_df['shop'] == 'aboutyou']\n",
    "aboutyou_prod_training['brand'] = aboutyou_prod_training['brand']\\\n",
    "                    .apply(lambda x: tt.processed_text(x))\n",
    "aboutyou_prod_training['title'] = aboutyou_prod_training['title']\\\n",
    "                    .apply(lambda x: titlet.processed_text(x))\n",
    "aboutyou_prod_training['color'] = aboutyou_prod_training['color']\\\n",
    "                    .apply(lambda x: ct.processed_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676ca9b",
   "metadata": {},
   "source": [
    "### BM25 setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a52ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRelevance:\n",
    "    def __init__(self, text, relevance):\n",
    "        self.text = text\n",
    "        self.relevance = relevance\n",
    "    def __repr__(self):\n",
    "        return self.text + ' ' + str(self.relevance)\n",
    "    def __eq__(self, other):\n",
    "        return self.text == other.text\n",
    "    def __hash__(self):\n",
    "        return hash(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Z(BM25L):\n",
    "    def __init__(self, corpus):\n",
    "        super().__init__(self.process_list(corpus))\n",
    "        self.corpus = self.process_list(corpus)\n",
    "    def process_list(self, titles_list):\n",
    "        return [text.split() for text in titles_list if text is not None]\n",
    "    def processed_text(self, text):\n",
    "        return tt.processed_text(text)\n",
    "    def get_corpus_str(self):\n",
    "        return [' '.join(el) for el in self.corpus]\n",
    "    def get_corpus_scores(self, query):\n",
    "        query = self.processed_text(query).split()\n",
    "        return [TextRelevance(x,y) for x, y in zip(self.get_corpus_str(), self.get_scores(query))]\n",
    "    def get_relevant_results(self, query, threshold=0):\n",
    "        res = [el for el in self.get_corpus_scores(query) if el.relevance > threshold]\n",
    "        res = sorted(res, key=lambda x: x.relevance, reverse=True)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77654c96",
   "metadata": {},
   "source": [
    "### Getting all non matching titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_not_matching_offers = aboutyou_prod_training.loc[\n",
    "    aboutyou_prod_training['offer_id'].isin(title_not_matched['aboutyou'])\n",
    "]\n",
    "bm25matcher_title = BM25Z(list(title_not_matching_offers['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb1d6e3",
   "metadata": {},
   "source": [
    "### Identify common substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_substring(str_1, str_2):\n",
    "    to_split = str_1 if len(str_1) < len(str_2) else str_2\n",
    "    to_comp = str_1 if len(str_1) > len(str_2) else str_2\n",
    "    if len(to_split) < 3 or len(to_comp) < 3:\n",
    "        return None\n",
    "    sub = all_substr(to_split)\n",
    "    for el in sub:\n",
    "        if el in to_comp:\n",
    "            return el\n",
    "    return None\n",
    "        \n",
    "def next_sub_substr(to_split):\n",
    "    l = len(to_split)\n",
    "    return to_split[:l-1] \n",
    "\n",
    "def next_sum_substr(to_split):\n",
    "    l = len(to_split)\n",
    "    return to_split[1:] \n",
    "\n",
    "def all_sub_substr(to_split):\n",
    "    l = [to_split]\n",
    "    for i in range(len(to_split) - 3):\n",
    "        to_split = next_sub_substr(to_split)\n",
    "        l += [to_split]\n",
    "    return l\n",
    "\n",
    "def all_substr(to_split):\n",
    "    l = all_sub_substr(to_split)\n",
    "    size = len(to_split) - 3\n",
    "    for i in range(size):\n",
    "        to_split = next_sum_substr(to_split)\n",
    "        l += all_sub_substr(to_split)\n",
    "    return sorted(l, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f43f4",
   "metadata": {},
   "source": [
    "### Mine common substrings\n",
    "e.g. Canvasgürtel, Ledergürtel -> gürtel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_common_terms():\n",
    "    common_terms = []\n",
    "    for i in range(len(title_not_matched)):\n",
    "        z_id = title_not_matched.loc[i]['zalando']\n",
    "        a_id = title_not_matched.loc[i]['aboutyou']\n",
    "        z_ti = zalando_prod_training.loc[zalando_prod_training['offer_id'] == z_id]['title'].values[0]\n",
    "        a_ti = aboutyou_prod_training.loc[aboutyou_prod_training['offer_id'] == a_id]['title'].values[0]\n",
    "        rel_titles = [el.text for el in bm25matcher_title.get_relevant_results(z_ti)]\n",
    "        if a_ti not in rel_titles:\n",
    "            com = longest_substring(z_ti, a_ti)\n",
    "            common_terms += [com] if com is not None else []\n",
    "    common_terms = [el for el in common_terms if ' ' not in el]\n",
    "    filter_common_terms = []\n",
    "    for el in common_terms:\n",
    "        for term in common_terms:\n",
    "            if el in term and len(el) < len(term):\n",
    "                filter_common_terms += [el]\n",
    "    to_rem = set(filter_common_terms)\n",
    "    common_terms = [el for el in common_terms if el not in to_rem]\n",
    "    return set(common_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_updated = mine_common_terms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a654679",
   "metadata": {},
   "source": [
    "### These terms will be added to a list to read from\n",
    "primary_clothes_de.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in common_updated:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de3ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
